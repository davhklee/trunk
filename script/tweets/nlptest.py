# -*- coding: utf-8 -*-
"""nlptest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gmUnfoGxYP9JpZhUaOjopem5Vg83pwlX
"""

#install tensorflow and keras
!pip uninstall tensorflow tensorflow-gpu tensorflow-text protocol --yes
!pip install tensorflow==2.17.0
!pip install tensorflow-text==2.17.0
!pip install tensorflow-gpu==2.12.0

!pip uninstall keras --yes
!pip install --upgrade keras-nlp
!pip install --upgrade keras
!pip install --upgrade keras-nlp-nightly

#install kgptalkie
!pip uninstall openai --yes
!pip uninstall langsmith --yes
!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall
!pip install googletrans

#python modules imports
import os
os.environ['KERAS_BACKEND'] = 'tensorflow'

import numpy as np
import pandas as pd
import tensorflow as tf
#import keras_core as keras
import keras
import keras_nlp
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import preprocess_kgptalkie as kgp

print("TensorFlow version:", tf.__version__)
print("Keras version:", keras.__version__)
print("KerasNLP version:", keras_nlp.__version__)

#import data
df_train = pd.read_csv("train.csv")
df_test = pd.read_csv("test.csv")
print("Training dim = {} memory = {:.2f} MB".format(df_train.shape, df_train.memory_usage().sum() / 1024 ** 2))
print("Test dim = {} memory = {:.2f} MB".format(df_test.shape, df_test.memory_usage().sum() / 1024 ** 2))

df_train["length"] = df_train["text"].apply(lambda x : len(x))
df_test["length"] = df_test["text"].apply(lambda x : len(x))

print("Train Length Stat")
print(df_train["length"].describe())

print("Test Length Stat")
print(df_test["length"].describe())

df_train.head()
df_test.head()

#data cleaning
def get_clean(x):
  x = str(x).lower().replace("\\","  ").replace("_", " ").replace(".", " ")
  #x = kgp.cont_exp(x)
  x = kgp.remove_emails(x)
  x = kgp.remove_urls(x)
  x = kgp.remove_html_tags(x)
  x = kgp.remove_rt(x)
  x = kgp.remove_accented_chars(x)
  x = kgp.remove_special_chars(x)
  #x = kgp.remove_dups_char(x)
  return x

df_train["text"] = df_train["text"].apply(lambda x: get_clean(x))
df_test["text"] = df_test["text"].apply(lambda x: get_clean(x))

df_train.head()
df_test.head()

#training with DistilBERT model
BATCH_SIZE = 32
NUM_TRAINING_EXAMPLES = df_train.shape[0]
TRAIN_SPLIT = 0.8
VAL_SPLIT = 0.2
STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES) * TRAIN_SPLIT // BATCH_SIZE
EPOCHS = 1
AUTO = tf.data.experimental.AUTOTUNE

X = df_train["text"]
y = df_train["target"]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)
X_test = df_test["text"]

preset = "distil_bert_base_en_uncased"

preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset, sequence_length=160, name="processor_4_tweets")

classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset, preprocessor = preprocessor, num_classes=2)

classifier.summary()

classifier.compile(loss = "sparse_categorical_crossentropy", optimizer = "adam", metrics = ["accuracy"])

history = classifier.fit(x = X_train, y = y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_val, y_val))

#inference with DistilBERT model
y_pred_train = classifier.predict(X_train)

#f1-score with DistilBERT model
print(y_pred_train)

#training with CNN model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D

#tokenize text
text = df_train["text"]
token = Tokenizer()
token.fit_on_texts(text)
vocab_size = len(token.word_index) + 1
encoded_text = token.texts_to_sequences(text) #turn words into indexed tokens
print("encoded_text:", encoded_text)
max_length = 40
X = pad_sequences(encoded_text, maxlen=max_length, padding="post")
print("X:", X)
y = df_train["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

#construct CNN network
vec_size = 100
model = Sequential()
model.add(Embedding(vocab_size, vec_size, input_length=max_length))

model.add(Conv1D(32, 1, activation="relu"))
model.add(MaxPooling1D(2))
model.add(Dropout(0.5))

model.add(Dense(32, activation="relu"))
model.add(Dropout(0.5))

model.add(Dense(16, activation="relu"))

model.add(GlobalMaxPooling1D())

model.add(Dense(1, activation="sigmoid"))

model.summary()

#fitting the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

#inference with CNN model
def get_encoded(x):
  x = get_clean(x)
  x = token.texts_to_sequences([x])
  x = pad_sequences(x, maxlen=max_length, padding="post")
  return x

y_pred_test = model.predict(X_test)
y_pred_test = y_pred_test > 0.4
X_val = get_encoded(X_test)
print("y_pred_test:", y_pred_test)
print("y_test:", y_test)

#f1-score with CNN model